{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZTxkaLmCd6r",
        "outputId": "df2c82a8-23b1-4f41-b0ef-ed50333d93b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Preprocessing complete. Processed data saved to /content/drive/My Drive/SneakerPricePrediction/processed_sneaker_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "input_path = '/content/drive/My Drive/StockX-Data-Contest-2019-3.csv'\n",
        "output_dir = '/content/drive/My Drive/SneakerPricePrediction/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "output_csv = os.path.join(output_dir, 'processed_sneaker_data.csv')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Clean 'Sale Price' and 'Retail Price' (remove $ and convert to float)\n",
        "df['Sale Price'] = df['Sale Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "df['Retail Price'] = df['Retail Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=['Order Date', 'Release Date', 'Sale Price', 'Retail Price', 'Shoe Size', 'Buyer Region', 'Sneaker Name', 'Brand'])\n",
        "\n",
        "# Convert date columns to datetime\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'], format='%m/%d/%y', errors='coerce')\n",
        "df['Release Date'] = pd.to_datetime(df['Release Date'], format='%m/%d/%y', errors='coerce')\n",
        "\n",
        "# Drop rows with invalid dates\n",
        "df = df.dropna(subset=['Order Date', 'Release Date'])\n",
        "\n",
        "# Validate dates (ensure Order Date >= Release Date)\n",
        "df = df[df['Order Date'] >= df['Release Date']]\n",
        "\n",
        "# Sort by Order Date for time-based split\n",
        "df = df.sort_values('Order Date')\n",
        "\n",
        "# Feature engineering\n",
        "# Days since release\n",
        "df['Days_Since_Release'] = (df['Order Date'] - df['Release Date']).dt.days\n",
        "\n",
        "# Extract year and month from Release Date\n",
        "df['Release_Year'] = df['Release Date'].dt.year\n",
        "df['Release_Month'] = df['Release Date'].dt.month\n",
        "\n",
        "# New features\n",
        "# Regional sales volume (count of sales per region)\n",
        "df['Regional_Sales_Volume'] = df.groupby('Buyer Region')['Buyer Region'].transform('count')\n",
        "\n",
        "# Sneaker rarity (inverse of sales volume per sneaker)\n",
        "df['Sneaker_Rarity'] = 1 / df.groupby('Sneaker Name')['Sneaker Name'].transform('count')\n",
        "\n",
        "# Monthly average sale price and volatility (by order year-month)\n",
        "df['Order_YearMonth'] = df['Order Date'].dt.to_period('M')\n",
        "df['Monthly_Avg_Price'] = df.groupby('Order_YearMonth')['Sale Price'].transform('mean')\n",
        "df['Market_Volatility'] = df.groupby('Order_YearMonth')['Sale Price'].transform('std').fillna(df['Sale Price'].std())\n",
        "\n",
        "# Price trend (ratio of Monthly_Avg_Price to historical average)\n",
        "historical_avg_price = df['Sale Price'].mean()\n",
        "df['Price_Trend'] = df['Monthly_Avg_Price'] / historical_avg_price\n",
        "df = df.drop('Order_YearMonth', axis=1)\n",
        "\n",
        "# Order year\n",
        "df['Order_Year'] = df['Order Date'].dt.year\n",
        "\n",
        "# Sneaker age (days since release normalized, avoid division by zero)\n",
        "df['Sneaker_Age'] = df['Days_Since_Release'] / np.maximum(df['Order_Year'] - df['Release_Year'] + 1, 1)\n",
        "df['Sneaker_Age'] = df['Sneaker_Age'].clip(lower=0, upper=1000)\n",
        "\n",
        "# Sneaker hype (sales volume per sneaker relative to average)\n",
        "avg_sneaker_sales = df.groupby('Sneaker Name')['Sneaker Name'].transform('count').mean()\n",
        "df['Sneaker_Hype'] = df.groupby('Sneaker Name')['Sneaker Name'].transform('count') / avg_sneaker_sales\n",
        "\n",
        "# Validate for inf or NaN\n",
        "for col in ['Sneaker_Age', 'Price_Trend', 'Sneaker_Hype', 'Market_Volatility']:\n",
        "    if np.isinf(df[col]).any() or pd.isna(df[col]).any():\n",
        "        print(f\"Warning: {col} contains inf or NaN values\")\n",
        "        df = df[~np.isinf(df[col]) & ~pd.isna(df[col])]\n",
        "\n",
        "# Encode categorical variables\n",
        "le_brand = LabelEncoder()\n",
        "le_sneaker = LabelEncoder()\n",
        "le_region = LabelEncoder()\n",
        "\n",
        "df['Brand_Encoded'] = le_brand.fit_transform(df['Brand'])\n",
        "df['Sneaker_Name_Encoded'] = le_sneaker.fit_transform(df['Sneaker Name'])\n",
        "df['Buyer_Region_Encoded'] = le_region.fit_transform(df['Buyer Region'])\n",
        "\n",
        "# Select features for modeling, include additional columns for output\n",
        "features = [\n",
        "    'Retail Price', 'Shoe Size', 'Days_Since_Release', 'Release_Year', 'Release_Month',\n",
        "    'Brand_Encoded', 'Sneaker_Name_Encoded', 'Buyer_Region_Encoded',\n",
        "    'Regional_Sales_Volume', 'Sneaker_Rarity', 'Monthly_Avg_Price', 'Order_Year', 'Sneaker_Age', 'Price_Trend', 'Sneaker_Hype', 'Market_Volatility'\n",
        "]\n",
        "target = 'Sale Price'\n",
        "additional_columns = ['Order Date', 'Sneaker Name', 'Brand', 'Release Date', 'Shoe Size']\n",
        "\n",
        "# Create processed dataset\n",
        "processed_df = df[features + additional_columns + [target]]\n",
        "\n",
        "# Save processed dataset\n",
        "processed_df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Save encoders for app usage\n",
        "with open(os.path.join(output_dir, 'le_brand.pkl'), 'wb') as f:\n",
        "    pickle.dump(le_brand, f)\n",
        "with open(os.path.join(output_dir, 'le_sneaker.pkl'), 'wb') as f:\n",
        "    pickle.dump(le_sneaker, f)\n",
        "with open(os.path.join(output_dir, 'le_region.pkl'), 'wb') as f:\n",
        "    pickle.dump(le_region, f)\n",
        "\n",
        "print(f\"Preprocessing complete. Processed data saved to {output_csv}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "input_csv = '/content/drive/My Drive/SneakerPricePrediction/processed_sneaker_data.csv'\n",
        "output_dir = '/content/drive/My Drive/SneakerPricePrediction/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "model_path = os.path.join(output_dir, 'xgboost_sneaker_model.json')\n",
        "model_random_path = os.path.join(output_dir, 'xgboost_sneaker_model_random.json')\n",
        "shap_summary_path = os.path.join(output_dir, 'shap_summary.png')\n",
        "shap_importance_path = os.path.join(output_dir, 'shap_importance.png')\n",
        "actual_vs_predicted_path = os.path.join(output_dir, 'actual_vs_predicted.png')\n",
        "error_over_time_path = os.path.join(output_dir, 'error_over_time.png')\n",
        "error_histogram_path = os.path.join(output_dir, 'error_histogram.png')\n",
        "residual_plot_path = os.path.join(output_dir, 'residual_plot.png')\n",
        "error_boxplot_path = os.path.join(output_dir, 'error_boxplot.png')\n",
        "predicted_vs_actual_line_path = os.path.join(output_dir, 'predicted_vs_actual_line.png')\n",
        "predicted_prices_path = os.path.join(output_dir, 'predicted_sneaker_prices.csv')\n",
        "\n",
        "# Load processed dataset\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Clean Brand column (remove leading/trailing spaces)\n",
        "df['Brand'] = df['Brand'].str.strip()\n",
        "\n",
        "# Define features and target\n",
        "features = [\n",
        "    'Retail Price', 'Shoe Size', 'Days_Since_Release', 'Release_Year', 'Release_Month',\n",
        "    'Brand_Encoded', 'Sneaker_Name_Encoded', 'Buyer_Region_Encoded',\n",
        "    'Regional_Sales_Volume', 'Sneaker_Rarity', 'Monthly_Avg_Price', 'Order_Year', 'Sneaker_Age', 'Price_Trend', 'Sneaker_Hype', 'Market_Volatility'\n",
        "]\n",
        "target = 'Sale Price'\n",
        "\n",
        "# Validate for inf or NaN in features\n",
        "for col in features:\n",
        "    if np.isinf(df[col]).any() or pd.isna(df[col]).any():\n",
        "        print(f\"Warning: {col} contains inf or NaN values\")\n",
        "        df = df[~np.isinf(df[col]) & ~pd.isna(df[col])]\n",
        "\n",
        "# Time-based train-test split (80% train, 20% test)\n",
        "train_size = int(0.8 * len(df))\n",
        "X_train = df[features].iloc[:train_size]\n",
        "y_train = df[target].iloc[:train_size]\n",
        "X_test = df[features].iloc[train_size:]\n",
        "y_test = df[target].iloc[train_size:]\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 300]\n",
        "}\n",
        "base_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    tree_method='hist',\n",
        "    device='cuda',\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=4.0,  # L1 regularization\n",
        "    reg_lambda=4.0,  # L2 regularization\n",
        "    random_state=42\n",
        ")\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "model = grid_search.best_estimator_\n",
        "\n",
        "# Cross-validation (5-fold)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "mae_scores, rmse_scores, r2_scores = [], [], []\n",
        "for train_idx, val_idx in kf.split(X_train):\n",
        "    X_cv_train, X_cv_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "    model.fit(X_cv_train, y_cv_train)\n",
        "    y_cv_pred = model.predict(X_cv_val)\n",
        "    mae_scores.append(mean_absolute_error(y_cv_val, y_cv_pred))\n",
        "    rmse_scores.append(np.sqrt(mean_squared_error(y_cv_val, y_cv_pred)))\n",
        "    r2_scores.append(r2_score(y_cv_val, y_cv_pred))\n",
        "\n",
        "print(\"Cross-Validation Results (Time-Based Split):\")\n",
        "print(f\"Mean MAE: {np.mean(mae_scores):.2f} ± {np.std(mae_scores):.2f}\")\n",
        "print(f\"Mean RMSE: {np.mean(rmse_scores):.2f} ± {np.std(rmse_scores):.2f}\")\n",
        "print(f\"Mean R²: {np.mean(r2_scores):.2f} ± {np.std(r2_scores):.2f}\")\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics on test set\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nTest Set Results (Time-Based Split):\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.2f}\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Random split for comparison\n",
        "X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(\n",
        "    df[features], df[target], test_size=0.2, random_state=42\n",
        ")\n",
        "grid_search.fit(X_train_rand, y_train_rand)\n",
        "model_rand = grid_search.best_estimator_\n",
        "y_pred_rand = model_rand.predict(X_test_rand)\n",
        "mae_rand = mean_absolute_error(y_test_rand, y_pred_rand)\n",
        "rmse_rand = np.sqrt(mean_squared_error(y_test_rand, y_pred_rand))\n",
        "r2_rand = r2_score(y_test_rand, y_pred_rand)\n",
        "\n",
        "print(\"\\nTest Set Results (Random Split):\")\n",
        "print(f\"MAE: {mae_rand:.2f}\")\n",
        "print(f\"RMSE: {rmse_rand:.2f}\")\n",
        "print(f\"R²: {r2_rand:.2f}\")\n",
        "print(f\"Best Parameters (Random Split): {grid_search.best_params_}\")\n",
        "\n",
        "# Actual vs. Predicted Scatter Plot (Time-Based Split)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, s=20)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual Sale Price ($)')\n",
        "plt.ylabel('Predicted Sale Price ($)')\n",
        "plt.title('Actual vs. Predicted Sneaker Sale Prices (Time-Based Split)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(actual_vs_predicted_path)\n",
        "plt.close()\n",
        "\n",
        "# Predicted vs. Actual Line Plot (Time-Based Split)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test[:100].values, label=\"Actual\", color=\"blue\")\n",
        "plt.plot(y_pred[:100], label=\"Predicted\", color=\"orange\", linestyle=\"--\")\n",
        "plt.title(\"Actual vs Predicted Sale Prices (First 100 Samples)\")\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Price (Dollars)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(predicted_vs_actual_line_path)\n",
        "plt.close()\n",
        "\n",
        "# Error over Time Plot (Time-Based Split)\n",
        "errors = np.abs(y_test - y_pred)\n",
        "order_dates = pd.to_datetime(df['Order Date'].iloc[train_size:])\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(order_dates, errors, alpha=0.5, s=20)\n",
        "plt.xlabel('Order Date')\n",
        "plt.ylabel('Absolute Prediction Error ($)')\n",
        "plt.title('Prediction Error Over Time (Time-Based Split)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(error_over_time_path)\n",
        "plt.close()\n",
        "\n",
        "# Error Histogram (Time-Based Split)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(errors, bins=50, edgecolor='k')\n",
        "plt.xlabel('Absolute Prediction Error ($)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Prediction Errors (Time-Based Split)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(error_histogram_path)\n",
        "plt.close()\n",
        "\n",
        "# Residual Plot (Time-Based Split)\n",
        "residuals = y_pred - y_test\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5, s=20)\n",
        "plt.axhline(0, color='r', linestyle='--', lw=2)\n",
        "plt.xlabel('Predicted Sale Price ($)')\n",
        "plt.ylabel('Residuals ($)')\n",
        "plt.title('Residual Plot (Time-Based Split)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(residual_plot_path)\n",
        "plt.close()\n",
        "\n",
        "# Error Boxplot by Year (Time-Based Split)\n",
        "plt.figure(figsize=(10, 6))\n",
        "test_df = pd.DataFrame({'Order_Year': df['Order_Year'].iloc[train_size:], 'Error': errors})\n",
        "test_df.boxplot(column='Error', by='Order_Year')\n",
        "plt.xlabel('Order Year')\n",
        "plt.ylabel('Absolute Prediction Error ($)')\n",
        "plt.title('Prediction Error by Year (Time-Based Split)')\n",
        "plt.suptitle('')\n",
        "plt.tight_layout()\n",
        "plt.savefig(error_boxplot_path)\n",
        "plt.close()\n",
        "\n",
        "# SHAP feature importance (Time-Based Split)\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# SHAP summary plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(shap_values, X_test, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(shap_summary_path)\n",
        "plt.close()\n",
        "\n",
        "# SHAP bar plot for feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(shap_values, X_test, plot_type='bar', show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(shap_importance_path)\n",
        "plt.close()\n",
        "\n",
        "# Predictions and SHAP values for entire dataset (Random Split Model for App)\n",
        "y_full_pred = model_rand.predict(df[features])\n",
        "explainer_rand = shap.TreeExplainer(model_rand)\n",
        "shap_values_full = explainer_rand.shap_values(df[features])\n",
        "\n",
        "# Map feature names to app terminology\n",
        "feature_mapping = {\n",
        "    'Sneaker_Rarity': 'Rarity',\n",
        "    'Sneaker_Hype': 'Demand',\n",
        "    'Days_Since_Release': 'time',\n",
        "    'Retail Price': 'Retail Price',\n",
        "    'Shoe Size': 'Shoe Size',\n",
        "    'Release_Year': 'Release Year',\n",
        "    'Release_Month': 'Release Month',\n",
        "    'Brand_Encoded': 'Brand',\n",
        "    'Sneaker_Name_Encoded': 'Sneaker Name',\n",
        "    'Buyer_Region_Encoded': 'Buyer Region',\n",
        "    'Regional_Sales_Volume': 'Regional Sales Volume',\n",
        "    'Monthly_Avg_Price': 'Monthly Avg Price',\n",
        "    'Order_Year': 'Order Year',\n",
        "    'Sneaker_Age': 'Sneaker Age',\n",
        "    'Price_Trend': 'Price Trend',\n",
        "    'Market_Volatility': 'Market Volatility'\n",
        "}\n",
        "\n",
        "# Prioritize Rarity, Demand, and time for SHAP features\n",
        "priority_features = ['Sneaker_Rarity', 'Sneaker_Hype', 'Days_Since_Release']\n",
        "shap_df = pd.DataFrame(shap_values_full, columns=features)\n",
        "top_features = []\n",
        "top_impacts = []\n",
        "for i in range(len(shap_df)):\n",
        "    shap_row = shap_df.iloc[i].copy()\n",
        "    # Get absolute SHAP values for sorting\n",
        "    abs_shap_row = shap_row.abs()\n",
        "    # Initialize top 3 with priority features if they exist\n",
        "    selected_features = []\n",
        "    selected_impacts = []\n",
        "    for feat in priority_features:\n",
        "        if feat in features:\n",
        "            selected_features.append(feature_mapping[feat])\n",
        "            selected_impacts.append(shap_row[feat])\n",
        "    # Fill remaining slots with highest absolute SHAP values\n",
        "    remaining_features = [f for f in features if f not in priority_features]\n",
        "    remaining_shap = abs_shap_row[remaining_features].sort_values(ascending=False)\n",
        "    for feat in remaining_shap.index:\n",
        "        if len(selected_features) < 3:\n",
        "            selected_features.append(feature_mapping[feat])\n",
        "            selected_impacts.append(shap_row[feat])\n",
        "    top_features.append(selected_features[:3])\n",
        "    top_impacts.append(selected_impacts[:3])\n",
        "\n",
        "# Create output DataFrame\n",
        "df_pred = pd.DataFrame({\n",
        "    'Sneaker Name': df['Sneaker Name'],\n",
        "    'Brand': df['Brand'],\n",
        "    'Retail Price': df['Retail Price'].round(2),\n",
        "    'Sale Price': df[target].round(2),\n",
        "    'Predicted Price': y_full_pred.round(2),\n",
        "    'Order Date': df['Order Date'],\n",
        "    'Release Date': df['Release Date'],\n",
        "    'Shoe Size': df['Shoe Size'].round(1),\n",
        "    'SHAP Feature 1': [f[0] for f in top_features],\n",
        "    'SHAP Impact 1': [round(i[0], 4) for i in top_impacts],\n",
        "    'SHAP Feature 2': [f[1] for f in top_features],\n",
        "    'SHAP Impact 2': [round(i[1], 4) for i in top_impacts],\n",
        "    'SHAP Feature 3': [f[2] for f in top_features],\n",
        "    'SHAP Impact 3': [round(i[2], 4) for i in top_impacts]\n",
        "})\n",
        "\n",
        "# Save predicted prices with SHAP data\n",
        "df_pred.to_csv(predicted_prices_path, index=False)\n",
        "\n",
        "# Save models\n",
        "model.save_model(model_path)  # Time-based model\n",
        "model_rand.save_model(model_random_path)  # Random split model\n",
        "\n",
        "print(f\"Training complete. Time-based model saved to {model_path}. Random split model saved to {model_random_path}.\")\n",
        "print(f\"Plots saved to {shap_summary_path}, {shap_importance_path}, {actual_vs_predicted_path}, {error_over_time_path}, {error_histogram_path}, {residual_plot_path}, {error_boxplot_path}, {predicted_vs_actual_line_path}.\")\n",
        "print(f\"Predicted prices saved to {predicted_prices_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "6yjkXiAJDuqS",
        "outputId": "e3406870-b339-4739-a86d-fdcc5e957c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cross-Validation Results (Time-Based Split):\n",
            "Mean MAE: 20.82 ± 0.34\n",
            "Mean RMSE: 40.58 ± 1.25\n",
            "Mean R²: 0.98 ± 0.00\n",
            "\n",
            "Test Set Results (Time-Based Split):\n",
            "MAE: 90.77\n",
            "RMSE: 149.79\n",
            "R²: 0.43\n",
            "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
            "\n",
            "Test Set Results (Random Split):\n",
            "MAE: 16.37\n",
            "RMSE: 31.23\n",
            "R²: 0.98\n",
            "Best Parameters (Random Split): {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300}\n",
            "Training complete. Time-based model saved to /content/drive/My Drive/SneakerPricePrediction/xgboost_sneaker_model.json. Random split model saved to /content/drive/My Drive/SneakerPricePrediction/xgboost_sneaker_model_random.json.\n",
            "Plots saved to /content/drive/My Drive/SneakerPricePrediction/shap_summary.png, /content/drive/My Drive/SneakerPricePrediction/shap_importance.png, /content/drive/My Drive/SneakerPricePrediction/actual_vs_predicted.png, /content/drive/My Drive/SneakerPricePrediction/error_over_time.png, /content/drive/My Drive/SneakerPricePrediction/error_histogram.png, /content/drive/My Drive/SneakerPricePrediction/residual_plot.png, /content/drive/My Drive/SneakerPricePrediction/error_boxplot.png, /content/drive/My Drive/SneakerPricePrediction/predicted_vs_actual_line.png.\n",
            "Predicted prices saved to /content/drive/My Drive/SneakerPricePrediction/predicted_sneaker_prices.csv.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "input_csv = '/content/drive/My Drive/SneakerPricePrediction/predicted_sneaker_prices.csv'\n",
        "output_json = '/content/drive/My Drive/SneakerPricePrediction/predicted_sneaker_prices.json'\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Convert dates to string format (YYYY-MM-DD)\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date']).dt.strftime('%Y-%m-%d')\n",
        "df['Release Date'] = pd.to_datetime(df['Release Date']).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Ensure numeric columns are properly formatted\n",
        "numeric_columns = ['Retail Price', 'Sale Price', 'Predicted Price', 'Shoe Size', 'SHAP Impact 1', 'SHAP Impact 2', 'SHAP Impact 3']\n",
        "for col in numeric_columns:\n",
        "    df[col] = df[col].astype(float)\n",
        "\n",
        "# Convert DataFrame to list of dictionaries\n",
        "json_data = df.to_dict(orient='records')\n",
        "\n",
        "# Save to JSON file\n",
        "with open(output_json, 'w') as f:\n",
        "    json.dump(json_data, f, indent=2)\n",
        "\n",
        "print(f\"JSON file saved to {output_json}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM4oJ1OaX5Pq",
        "outputId": "77441d0d-0b43-4142-80e1-84aa76bbad10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "JSON file saved to /content/drive/My Drive/SneakerPricePrediction/predicted_sneaker_prices.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "input_csv = '/content/drive/My Drive/StockX-Data-Contest-2019-3.csv'\n",
        "output_dir = '/content/drive/My Drive/SneakerPricePrediction/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "sale_price_hist_path = os.path.join(output_dir, 'sale_price_histogram.png')\n",
        "brand_boxplot_path = os.path.join(output_dir, 'brand_sale_price_boxplot.png')\n",
        "correlation_matrix_path = os.path.join(output_dir, 'correlation_matrix.png')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Clean 'Sale Price' and 'Retail Price' (remove $ and convert to float)\n",
        "df['Sale Price'] = df['Sale Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "df['Retail Price'] = df['Retail Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Convert date columns to datetime\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'], format='%m/%d/%y', errors='coerce')\n",
        "df['Release Date'] = pd.to_datetime(df['Release Date'], format='%m/%d/%y', errors='coerce')\n",
        "\n",
        "# Drop rows with invalid dates\n",
        "df = df.dropna(subset=['Order Date', 'Release Date'])\n",
        "\n",
        "# Days since release\n",
        "df['Days_Since_Release'] = (df['Order Date'] - df['Release Date']).dt.days\n",
        "\n",
        "# Histogram of Sale Price\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['Sale Price'], bins=50, edgecolor='k')\n",
        "plt.xlabel('Sale Price ($)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Sneaker Sale Prices')\n",
        "plt.tight_layout()\n",
        "plt.savefig(sale_price_hist_path)\n",
        "plt.close()\n",
        "\n",
        "# Box Plot of Sale Price by Brand\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Brand', y='Sale Price', data=df)\n",
        "plt.xlabel('Brand')\n",
        "plt.ylabel('Sale Price ($)')\n",
        "plt.title('Sale Price Distribution by Brand')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(brand_boxplot_path)\n",
        "plt.close()\n",
        "\n",
        "# Correlation Matrix of Numeric Features\n",
        "numeric_cols = ['Sale Price', 'Retail Price', 'Shoe Size', 'Days_Since_Release']\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix of Numeric Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig(correlation_matrix_path)\n",
        "plt.close()\n",
        "\n",
        "print(f\"Charts saved to {sale_price_hist_path}, {brand_boxplot_path}, {correlation_matrix_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BirlSrP6WD58",
        "outputId": "b0964fa5-b6a4-423b-c0ae-3fdb31657c1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Charts saved to /content/drive/My Drive/SneakerPricePrediction/sale_price_histogram.png, /content/drive/My Drive/SneakerPricePrediction/brand_sale_price_boxplot.png, /content/drive/My Drive/SneakerPricePrediction/correlation_matrix.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "input_csv = '/content/drive/My Drive/StockX-Data-Contest-2019-3.csv'\n",
        "output_dir = '/content/drive/My Drive/SneakerPricePrediction/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "sorted_csv = os.path.join(output_dir, 'sorted_sale_prices.csv')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# Clean 'Sale Price' (remove $ and convert to float)\n",
        "df['Sale Price'] = df['Sale Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df = df.dropna(subset=['Sale Price'])\n",
        "\n",
        "# Find min and max Sale Price\n",
        "min_sale_price = df['Sale Price'].min()\n",
        "max_sale_price = df['Sale Price'].max()\n",
        "\n",
        "# Sort the dataset by Sale Price (ascending)\n",
        "df_sorted = df.sort_values(by='Sale Price', ascending=True)\n",
        "\n",
        "# Save the sorted dataset to a new CSV\n",
        "df_sorted.to_csv(sorted_csv, index=False)\n",
        "\n",
        "# Print results\n",
        "print(f\"Minimum Sale Price: ${min_sale_price:.2f}\")\n",
        "print(f\"Maximum Sale Price: ${max_sale_price:.2f}\")\n",
        "print(f\"Sorted dataset saved to {sorted_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBwrZ2QCzoga",
        "outputId": "2a482de9-4327-42d6-e1dd-813fc0e7cffa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Minimum Sale Price: $186.00\n",
            "Maximum Sale Price: $4050.00\n",
            "Sorted dataset saved to /content/drive/My Drive/SneakerPricePrediction/sorted_sale_prices.csv\n"
          ]
        }
      ]
    }
  ]
}